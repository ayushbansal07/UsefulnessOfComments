{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Input, concatenate, Dropout, Reshape\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import precision_recall_fscore_support as fscore\n",
    "from sklearn.metrics.pairwise import cosine_similarity as CS\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLEANING_PATTERSN = re.compile(\"[\\s\\n\\r\\t.,:;\\-_\\'\\\"?!#&()*]\")\n",
    "LSTM_HIDDEN_SIZE = 200\n",
    "MAX_TIME = 30 #MAXIMUM SIZE OF A COMMENT TO BE PASSED TO LSTM\n",
    "VOCAB_SIZE = 10000 #MAX VOCAB SIZE\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 2000\n",
    "FILE_TYPE = 'all' #should be one of 'all', 'ProgramDomain', 'ProblemDomain', 'ProjectManagement'\n",
    "MIDDLE_LAYER_ACTIVATION = 'relu' #Activation function in middle layers.\n",
    "FINAL_LAYER_ACTIVATION = 'sigmoid' #Activation function of final layer.\n",
    "K = 5 #Parameter for K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = pd.read_csv('DATA/GENERATED/TRAIN/Z_CONCATED_commentType.csv',delimiter='\\t') #Z contains the comment text\n",
    "FEATS = pd.read_csv('DATA/GENERATED/TRAIN/CONCATED_commentType_'+FILE_TYPE+'.csv') #Features for training\n",
    "FEATS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = np.array(Z['F2'])\n",
    "X = np.array(FEATS)[:,:12]\n",
    "if FILE_TYPE == 'all':\n",
    "    Y = np.array(FEATS[['ProgramDomain','ProjectManagement','ProblemDomain']])\n",
    "else:\n",
    "    Y = np.array(FEATS['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comments Cleaning\n",
    "ctr = Counter()\n",
    "mp = {}\n",
    "sentences = []\n",
    "for comment in comments:\n",
    "    sent = [x.strip() for x in CLEANING_PATTERSN.split(comment) if x!='']\n",
    "    ctr[len(sent)] += 1\n",
    "    sentences.append(sent)\n",
    "    if len(sent) not in mp:\n",
    "        mp[len(sent)] = []\n",
    "    mp[len(sent)].append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctr = Counter()\n",
    "for sent in sentences:\n",
    "    for word in sent:\n",
    "        ctr[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For creating a vocabulary and convert a sentence (vector of words) to vector of indices\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_sent are Comment texts to be passed for training. (Input to model)\n",
    "train_sent = tokenizer.texts_to_sequences(sentences)\n",
    "train_sent = pad_sequences(train_sent, maxlen=MAX_TIME,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if FILE_TYPE == 'all':\n",
    "    train_y = Y\n",
    "else:\n",
    "    train_y = to_categorical(Y)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "NUM_TRAIN = int(0.9*len(X))\n",
    "print(NUM_TRAIN)\n",
    "train_x = X[:NUM_TRAIN]\n",
    "test_x = X[NUM_TRAIN:]\n",
    "train_y, test_y = train_y[:NUM_TRAIN], train_y[NUM_TRAIN:]\n",
    "train_sent, test_sent = train_sent[:NUM_TRAIN], train_sent[NUM_TRAIN:]\n",
    "print(train_x.shape, train_y.shape, train_sent.shape, test_x.shape, test_y.shape, test_sent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide_into_k_folds(train_x, train_y, train_sent,k):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    sents = []\n",
    "    each = int(len(train_x)/k)\n",
    "    for i in range (k-1):\n",
    "        xs.append(train_x[i*each:(i+1)*each])\n",
    "        ys.append(train_y[i*each:(i+1)*each])\n",
    "        sents.append(train_sent[i*each:(i+1)*each])\n",
    "    xs.append(train_x[(k-1)*each:])\n",
    "    ys.append(train_y[(k-1)*each:])    \n",
    "    sents.append(train_sent[(k-1)*each:])    \n",
    "    return np.array(xs), np.array(ys), np.array(sents)\n",
    "\n",
    "def get_fold(train_x, train_y, train_sent,i,k):\n",
    "    ids = [x for x in range(k) if x != i]\n",
    "    print(i,k,ids)\n",
    "    return np.concatenate(train_x[ids],axis=0), np.concatenate(train_y[ids],axis=0), \\\n",
    "        np.concatenate(train_sent[ids],axis=0)\n",
    "\n",
    "def get_all_data_from_folds(train_x, train_y, train_sent):\n",
    "    return np.concatenate(train_x,axis=0), np.concatenate(train_y,axis = 0),\\\n",
    "            np.concatenate(train_sent,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, train_sent = divide_into_k_folds(train_x, train_y, train_sent, K)\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(optimizer='rmsprop',lr=LEARNING_RATE,middle_act=MIDDLE_LAYER_ACTIVATION,\n",
    "               final_act=FINAL_LAYER_ACTIVATION,dropout=DROPOUT,lstm_hidden=LSTM_HIDDEN_SIZE): \n",
    "    \n",
    "    sent_input = Input(shape=(MAX_TIME,)) #Input 1 - Comment text\n",
    "    extracted_feats = Input(shape=(12,)) #Input 2 - 12 Features\n",
    "    print(sent_input.shape, extracted_feats.shape)\n",
    "    \n",
    "    embeddingLayer = Embedding(VOCAB_SIZE, 100, input_length=MAX_TIME,  trainable=True)\n",
    "    sent = embeddingLayer(sent_input)\n",
    "    _, h1, c1 = LSTM(lstm_hidden,dropout=dropout,return_state=True)(sent) #Feed the comments to LSTM\n",
    "    print(h1.shape)\n",
    "    # Concat h1 and 12 features\n",
    "    feature_vector = concatenate([h1,extracted_feats],axis=1) #Concat output of LSTM with the 12 features\n",
    "    print(feature_vector.shape)\n",
    "    probs = Dense(64,activation=middle_act)(feature_vector) #Dense layer over LSTM_HIDEEN_SIZE + 12 features\n",
    "    print(probs.shape)\n",
    "    probs = Dense(3,activation=final_act)(probs) #Final Activation. Use sigmoid and NOT Softmax here.\n",
    "    print(probs.shape)\n",
    "    model = Model(inputs=[sent_input,extracted_feats],outputs=probs)\n",
    "    if optimizer == 'rmsprop':\n",
    "        optimizer = optimizers.rmsprop(lr=lr)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = optimizers.adam(lr=lr)\n",
    "    else:\n",
    "        print(\"Optimizer not supported!\")\n",
    "        return\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['binary_accuracy','categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find fscore for a model\n",
    "def find_fs(model):\n",
    "    predictions = model.predict([test_sent,test_x],batch_size=BATCH_SIZE)\n",
    "    if FILE_TYPE == 'all':\n",
    "        predictions = np.where(predictions > 0.5,1,0)\n",
    "    else:\n",
    "        predictions = predictions.argmax(axis=1)\n",
    "    if FILE_TYPE == 'all':\n",
    "        fs = fscore(test_y,predictions)\n",
    "    else:\n",
    "        fs = fscore(test_y.argmax(axis=1),predictions)\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run, takes parameters for model. Returns K-models from K-cross validation (We use only final one) \n",
    "# and Fscore Statistics from all of them.\n",
    "def run(optimizer='rmsprop',lr=LEARNING_RATE,middle_act=MIDDLE_LAYER_ACTIVATION,\n",
    "               final_act=FINAL_LAYER_ACTIVATION,dropout=DROPOUT,lstm_hidden=LSTM_HIDDEN_SIZE):\n",
    "    MODELS = []\n",
    "    FSS = []\n",
    "    for k in range(K):\n",
    "        print(\"-----------------Running Fold - \",k+1,\" of \",K,\"-------------------\")\n",
    "        model = build_model(optimizer,lr,middle_act,final_act,dropout,lstm_hidden)\n",
    "        MODELS.append(model)\n",
    "        curr_train_x, curr_train_y, curr_train_sent = get_fold(train_x, train_y, train_sent,k,K)\n",
    "        print(curr_train_x.shape)\n",
    "        model.fit([curr_train_sent,curr_train_x],curr_train_y,epochs=NUM_EPOCHS,batch_size=BATCH_SIZE,verbose=1,\n",
    "              validation_data=([train_sent[k], train_x[k]],train_y[k]))\n",
    "        FSS.append(find_fs(model))\n",
    "        model.save('model_'+FILE_TYPE+'_fold_'+str(k)+'.h5')\n",
    "    return MODELS, FSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO CONTINUE TRAINING FOR MORE EPOCHS\n",
    "# for k in range(K):\n",
    "#     print(\"-----------------Running Fold - \",k+1,\" of \",K,\"-------------------\")\n",
    "#     model = MODELS[k]\n",
    "#     model.fit([train_sent[k],train_x[k]],train_y[k],epochs=NUM_EPOCHS,batch_size=BATCH_SIZE,verbose=1,\n",
    "#           validation_data=([test_sent, test_x],test_y))\n",
    "#     model.save('model_'+FILE_TYPE+'_fold_'+str(k)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get predictions for an ensemble for models. \n",
    "def get_predictions(test_x, test_sent,models_arr=None):\n",
    "    prediction_scores = np.zeros((len(test_x),3))\n",
    "    k = len(models_arr)\n",
    "    for mod in models_arr:\n",
    "        predictions = mod.predict([test_sent, test_x],batch_size=BATCH_SIZE)\n",
    "        if FILE_TYPE == 'all':\n",
    "            predictions = np.where(predictions > 0.5,1,0)\n",
    "        else:\n",
    "            predictions = predictions.argmax(axis=1)\n",
    "        prediction_scores += predictions\n",
    "    print(prediction_scores)\n",
    "    return np.where(prediction_scores > k/2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions = get_predictions(test_x, test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if FILE_TYPE == 'all':\n",
    "#     fs = fscore(test_y,predictions)\n",
    "# else:\n",
    "#     fs = fscore(test_y.argmax(axis=1),predictions)\n",
    "# fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('model_'+FILE_TYPE+\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENSEMBLE_FSS = {} #Key - experiment name. Value - FScore Statistics of the experiment.\n",
    "if not os.path.exists('ensemble_models'):\n",
    "    os.mkdir('ensemble_models')\n",
    "if os.path.exists('LSTM_ENSEMBLE_MODELS_SUMMARY.map'):\n",
    "    with open('LSTM_ENSEMBLE_MODELS_SUMMARY.map','rb') as f:\n",
    "        ENSEMBLE_FSS = pickle.load(f)\n",
    "# Saves all the information for an experiment. Saves the FScore Stats in ENSEMBLE_FSS, \n",
    "# saves the models in folder ensemble_models, and dumps the ENSEMBLE_FSS to be read later.\\\n",
    "# Input parameters - MODELS as returned by run(), FSS as returned by run(), name of the experiment.\n",
    "def _put(m,f,name):\n",
    "    for j,model in enumerate(m):\n",
    "        model.save('ensemble_models/model_'+name+str(j)+'.h5')\n",
    "    ENSEMBLE_FSS[name] = f\n",
    "    with open('LSTM_ENSEMBLE_MODELS_SUMMARY.map','wb') as f:\n",
    "        pickle.dump(ENSEMBLE_FSS,f)\n",
    "# Running different experiments.\n",
    "\n",
    "# Default model\n",
    "m, f = run()\n",
    "_put(m,f,'default')\n",
    "# 2*LSTM_HIDDEN\n",
    "m, f = run(lstm_hidden=2*LSTM_HIDDEN_SIZE)\n",
    "_put(m,f,'2LSTM_HIDDEN')\n",
    "# 4*LSTM_HIDDEN\n",
    "m,f = run(lstm_hidden=4*LSTM_HIDDEN_SIZE)\n",
    "_put(m,f,'4LSTM_HIDDEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensemble Prediction\n",
    "with open('LSTM_ENSEMBLE_MODELS_SUMMARY.map','rb') as f:\n",
    "    ENSEMBLE_FSS = pickle.load(f)\n",
    "ENSEMBLE_MODELS = []\n",
    "for k,v in ENSEMBLE_FSS.items():\n",
    "    # Taking only last fold model\n",
    "    m = keras.models.load_model('ensemble_models/model_'+k+str(len(v)-1)+'.h5')\n",
    "    ENSEMBLE_MODELS.append(m)\n",
    "predictions = get_predictions(test_x, test_sent, ENSEMBLE_MODELS)\n",
    "if FILE_TYPE == 'all':\n",
    "    fs = fscore(test_y,predictions)\n",
    "else:\n",
    "    fs = fscore(test_y.argmax(axis=1),predictions)\n",
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Embeddings Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visulaising Embeddings\n",
    "embeddings = model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed(word):\n",
    "    return embeddings[tokenizer.word_index[word]].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_WORDS_FOR_ANALYSIS = 50\n",
    "SIM = []\n",
    "ALL_WORDS = []\n",
    "all_words = tokenizer.word_index.keys()\n",
    "for word in all_words:\n",
    "    ALL_WORDS.append(word)\n",
    "all_words = ALL_WORDS\n",
    "for i in range(NUM_WORDS_FOR_ANALYSIS):\n",
    "    for j in range(i+1,NUM_WORDS_FOR_ANALYSIS):\n",
    "        SIM.append((all_words[i],all_words[j],CS(embed(all_words[i]),embed(all_words[j]))[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SS = sorted(SIM,reverse=True,key=(lambda x:abs(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tsne_plot():\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in all_words[:50]:\n",
    "        tokens.append(embed(word)[0])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    #plt.show()\n",
    "    plt.savefig('SP.svg',format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
